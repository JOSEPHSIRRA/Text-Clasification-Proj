aComprehensive understanding of text classification using scikit learn, python and NLTK
Dataset that is considered for performing the analysis is sentiment 140 from kaggle. Link of the dataset----https://www.kaggle.com/kazanova/sentiment140
#!unzip '/content/drive/MyDrive/Text_data/Text_analysis.zip' -d '/content/drive/MyDrive/Text_data/'
Archive:  /content/drive/MyDrive/Text_data/Text_analysis.zip
  inflating: /content/drive/MyDrive/Text_data/training.1600000.processed.noemoticon.csv  
from google.colab import drive
drive.mount('/content/drive')
Mounted at /content/drive
Importing required libraries
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
import pandas as pd #importing library which is required to conduct this research.
import numpy as np
//uyguygu
import matplotlib.pyplot as plt
import tensorflow as tf
#Above library help in  performing the natural language processing, such as tokenization, lemmatization or more.
Data loading and preprocessing
text_data=pd.read_csv('/content/drive/MyDrive/Text_data/data.csv', header=None, encoding='latin-1')#reading csv file, or loading from local drive 
text_data.head()#showing top five data of the dataset
   0  ...                                                  5
0  0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...
1  0  ...  is upset that he can't update his Facebook by ...
2  0  ...  @Kenichan I dived many times for the ball. Man...
3  0  ...    my whole body feels itchy and like its on fire 
4  0  ...  @nationwideclass no, it's not behaving at all....

[5 rows x 6 columns]
text_data.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1600000 entries, 0 to 1599999
Data columns (total 6 columns):
 #   Column  Non-Null Count    Dtype 
---  ------  --------------    ----- 
 0   0       1600000 non-null  int64 
 1   1       1600000 non-null  int64 
 2   2       1600000 non-null  object
 3   3       1600000 non-null  object
 4   4       1600000 non-null  object
 5   5       1600000 non-null  object
dtypes: int64(2), object(4)
memory usage: 73.2+ MB
text_data.columns #showing columns.
Int64Index([0, 1, 2, 3, 4, 5], dtype='int64')
text_data.shape #Shape of the data (columns, rows available in the dataset)
(1600000, 6)
Performing Data Sampling
def sample(text_data): #function in order to perform sampling process over the dataset
    sample_data=min(100000, text_data[0].value_counts().min())
    data_label=text_data.groupby(0).apply(lambda i: i.sample(sample_data)) #grouping of data
    data_label.index=data_label.index.droplevel(0)
    return data_label
dataset=sample(text_data).reset_index(drop=True)#resetting the dataset
dataset.head()#showing the dataset after performing sampling.
   0  ...                                                  5
0  0  ...  once again i'm reminded how much i dislike sle...
1  0  ...  I wanna see The Hangover.  Lookin' forward to ...
2  0  ...         off to do the dreaded supermarket thing.  
3  0  ...  finished Transformers-9.5/10 it's worrying me ...
4  0  ...  @victoriax3jonas 1) that's really weird about ...

[5 rows x 6 columns]
Cleaning and removing, null and unnecessary data available in the dataset
cleaned_data=dataset.drop([2,4,3,1], axis=1) #dropping other data which is not necessary
cleaned_data.rename(columns={0:'Lables', 5:'tweets'}, inplace=True)#changing the names of columns
cleaned_data.head() #showing data after cleaning 
   Lables                                             tweets
0       0  once again i'm reminded how much i dislike sle...
1       0  I wanna see The Hangover.  Lookin' forward to ...
2       0         off to do the dreaded supermarket thing.  
3       0  finished Transformers-9.5/10 it's worrying me ...
4       0  @victoriax3jonas 1) that's really weird about ...
Data Preprocessing and Exploration of data
cleaned_data.isnull().sum()
Lables    0
tweets    0
dtype: int64
#making another copy of the dataset in order to perfoem a preprocess over the tweets.
df_tweets=cleaned_data.copy()
tweets=df_tweets.tweets.copy()
tweets # tweets data 
0         once again i'm reminded how much i dislike sle...
1         I wanna see The Hangover.  Lookin' forward to ...
2                off to do the dreaded supermarket thing.  
3         finished Transformers-9.5/10 it's worrying me ...
4         @victoriax3jonas 1) that's really weird about ...
                                ...                        
199995    Getting a surprise from the boyfriend...I can'...
199996    Cake boss and obsessed till bed . Only 3 more ...
199997    Talking sleeve caps with Norah is a total mind...
199998    Ah Sis(: We Get To Hang With Them Soon!  You &...
199999    @slpknt10l03 hey! *waves*. Thanks for following. 
Name: tweets, Length: 200000, dtype: object
Removing all the stopwords, auxilary verbs, urls and alphanumeric values
#removing all the alpha numeric charaters from the dataset using the below script. 
remove=[]
for i in tweets:
    text=i.lower()
    text=text.split()
    remove_list=['@', 'http','#', 'www', '/', '?', '...', '!']
    data_ = [te for te in text if all(c not in te for c in remove_list)]
    remove.append(' '.join(data_))
cleaned_data=remove #assigning values to new varibles
cleaned_data[0] #the sentence at the zeroth index
{"type":"string"}
#total length of the text 
len(cleaned_data)
200000
Performing Tokenization
#Tokenization using nltk library
tokens=[]
tokenize=nltk.RegexpTokenizer(r'\w+')
for word, v in enumerate(cleaned_data):
    v_lower=v.lower()
    values=tokenize.tokenize(v_lower)
    for i in range(len(values)-1, -1, -1):
        if len(values[i])<3:
            del(values[i])
    tokens.append(values)
tokens[0]
['once',
 'again',
 'reminded',
 'how',
 'much',
 'dislike',
 'sleeping',
 'rather',
 'trying',
 'sleep']
Removing stopwords
# assigning stopword as english
nltk.download('stopwords')
stopwords=set(stopwords.words('english')) # all the auxulary and helping verbs are removed.
filtered_tweets=[]
for i, j in enumerate(tokens):
    filters=[]
    for k in j:
        if k not in stopwords:
            filters.append(k)
    filtered_tweets.append(filters)
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
filtered_tweets[4] #filtered values 
['really',
 'weird',
 'joe',
 'really',
 'weird',
 'know',
 'shows',
 'except',
 'one',
 'makes',
 'sad']
Performing Lemmatization
nltk.download('wordnet')#downloading wordnet library 
lemma=WordNetLemmatizer()
text_data=[]
for i, j in enumerate(filtered_tweets):
    le=[]
    for k in j:
        le.append(lemma.lemmatize(k))#calling lemmatzation fucntion for get the basic words of the sentences 
    text_data.append(le)
[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data]   Unzipping corpora/wordnet.zip.
text_data[5]
['look', 'really', 'sorry', 'ment', 'insult', 'soo', 'forgive']
actual_data=[]
for i in text_data:
    s=' '.join(i)
    actual_data.append(s)#joining all the words in order to make sentence 
len(actual_data)
200000
actual_data[3:9] #designed data after cleaning and merging
['finished worrying many bad review second one',
 'really weird joe really weird know show except one make sad',
 'look really sorry ment insult soo forgive',
 'wtf cineworld midnight showing transformer',
 'know really trying everything could',
 'make freaking trade franchise']
df_tweets['tweets']=actual_data #assiging data to tweet varible in the dataframe
df_tweets.head()#showing top five data of the dataset.
   Lables                                             tweets
0       0  reminded much dislike sleeping rather trying s...
1       0         wanna see hangover lookin forward tomorrow
2       0                          dreaded supermarket thing
3       0       finished worrying many bad review second one
4       0  really weird joe really weird know show except...
df_tweets["Lables"].unique() #where 0 for false and 1 for true
array([0, 4])
df_tweets.shape #checking shape of the dataset, means row and columns present in the dataset
(200000, 2)
Designing model
from nltk.tokenize import TweetTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer #importing libraries that can be used for tokenizing the words 
#calling the tokenizer
token_function=TweetTokenizer() 
vect_word=TfidfVectorizer(ngram_range=(1,2), tokenizer=token_function.tokenize) # tfidif vectorizer help in checking the frequency of words
vect_word.fit(df_tweets['tweets'])#fitting the data to the model 
/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'
  "The parameter 'token_pattern' will not be used"
TfidfVectorizer(ngram_range=(1, 2),
                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fd4e7d69410>>)
train_data=vect_word.transform(df_tweets['tweets'])#transforming the data
test_data=df_tweets['Lables'] #assigning the lables as test data 
#splitting dataset into training and testing
x_train, x_test, y_train, y_test=train_test_split(train_data, test_data, test_size=0.245, random_state=10)
#Support Vactor Machine
from sklearn.svm import LinearSVC #importing library in order to perform standarization of data
model_svc=LinearSVC()
model_svc.fit(x_train, y_train) # linear svc model is designed 
LinearSVC()
from sklearn.metrics import accuracy_score
print(accuracy_score( model_svc.predict(x_test) , y_test )) # caluculated accuracy is around 75 percent 
0.7522244897959184
#Bagging Classifier
from sklearn.ensemble import BaggingClassifier
from sklearn.svm import SVC
bagg_class = BaggingClassifier(base_estimator=SVC(),n_estimators=10, random_state=0).fit(x_train[:20000], y_train[:20000])
print(accuracy_score(bagg_class.predict(x_test[:20000]), y_test[:20000]))
0.73365
#Boosting Classifier
 from sklearn.ensemble import GradientBoostingClassifier
boost_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)
boost_clf.fit(x_train[:20000], y_train[:20000])
GradientBoostingClassifier(learning_rate=1.0, max_depth=1)
boost_clf.score(x_test, y_test)
0.679530612244898
#Deep Neural Network
#changing the values into categorical form 
from tensorflow.keras.utils import to_categorical
labels=to_categorical(df_tweets['Lables'])
labels # creating categorcial data for the target inputs 
array([[1., 0., 0., 0., 0.],
       [1., 0., 0., 0., 0.],
       [1., 0., 0., 0., 0.],
       ...,
       [0., 0., 0., 0., 1.],
       [0., 0., 0., 0., 1.],
       [0., 0., 0., 0., 1.]], dtype=float32)
#Again splitting of data
x_train, x_test, y_train, y_test=train_test_split(df_tweets['tweets'], labels, test_size=0.35, random_state=10) 
#importing library for desgining deep neural network model 
from keras.models import Sequential
from keras.layers import Dense,Embedding
from tensorflow.keras.optimizers import Adam
from keras.layers import Dropout,Bidirectional,Conv1D,GlobalMaxPooling1D,MaxPooling1D
from keras.callbacks import Callback, EarlyStopping
# performing tokenization for making data into standard format that can be fitted to the model easily
from keras.preprocessing.text import Tokenizer
toke = Tokenizer(num_words=12000)
toke.fit_on_texts(list(x_train))#fitting training data 
from keras.preprocessing.sequence import pad_sequences #this help in checking that all the sequences in the list have the same length ot not
X_train=pad_sequences(toke.texts_to_sequences(x_train), maxlen=25)
X_test=pad_sequences(toke.texts_to_sequences(x_test), maxlen=25) #making sequcnes of words and the max length is taken as 25
print(X_train.shape)
print(X_test.shape)# checking shape of the splitted dataset
(130000, 25)
(70000, 25)
X_train.shape[1]
25
#Sequential model 
de_model=Sequential()
#embedding help in encoding the data into integer 
de_model.add(Embedding(12000, 80,input_length=X_train.shape[1])) # input layer 
de_model.add(Dropout(0.2)) # this help model from undergoing under fitting or over fitting
de_model.add(Conv1D(64, kernel_size=3, padding='same', strides=2,  
                    activation='relu'))# add convolution layer for processing the input 
de_model.add(GlobalMaxPooling1D())# same as max poolin which help in reducing the dimension of the array
de_model.add(Dense(128, activation='relu')) # dense layer for producing the output
de_model.add(Dropout(0.2))
de_model.add(Dense(5, activation='sigmoid')) # the filter is same as the numebr of features available in the dataset
de_model.summary() #summary of the model designed 
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (None, 25, 80)            960000    
                                                                 
 dropout (Dropout)           (None, 25, 80)            0         
                                                                 
 conv1d (Conv1D)             (None, 13, 64)            15424     
                                                                 
 global_max_pooling1d (Globa  (None, 64)               0         
 lMaxPooling1D)                                                  
                                                                 
 dense (Dense)               (None, 128)               8320      
                                                                 
 dropout_1 (Dropout)         (None, 128)               0         
                                                                 
 dense_1 (Dense)             (None, 5)                 645       
                                                                 
=================================================================
Total params: 984,389
Trainable params: 984,389
Non-trainable params: 0
_________________________________________________________________
de_model.compile(optimizer='adam', loss="categorical_crossentropy", metrics=['accuracy']) #compiling the model using optimizer and loss function
es=EarlyStopping(mode='min', monitor='loss', patience=3)#early stopping is used in order to stop the model undrgoing over and under fitting
history=de_model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=64, epochs=20, verbose=1, callbacks=[es]) # model training 
Epoch 1/20
2032/2032 [==============================] - 31s 9ms/step - loss: 0.5485 - accuracy: 0.7191 - val_loss: 0.5053 - val_accuracy: 0.7504
Epoch 2/20
2032/2032 [==============================] - 18s 9ms/step - loss: 0.4719 - accuracy: 0.7749 - val_loss: 0.5071 - val_accuracy: 0.7506
Epoch 3/20
2032/2032 [==============================] - 18s 9ms/step - loss: 0.4229 - accuracy: 0.8036 - val_loss: 0.5225 - val_accuracy: 0.7431
Epoch 4/20
2032/2032 [==============================] - 20s 10ms/step - loss: 0.3697 - accuracy: 0.8306 - val_loss: 0.5732 - val_accuracy: 0.7374
Epoch 5/20
2032/2032 [==============================] - 18s 9ms/step - loss: 0.3211 - accuracy: 0.8535 - val_loss: 0.6323 - val_accuracy: 0.7270
Epoch 6/20
2032/2032 [==============================] - 20s 10ms/step - loss: 0.2858 - accuracy: 0.8700 - val_loss: 0.7172 - val_accuracy: 0.7301
Epoch 7/20
2032/2032 [==============================] - 18s 9ms/step - loss: 0.2596 - accuracy: 0.8827 - val_loss: 0.8008 - val_accuracy: 0.7222
Epoch 8/20
2032/2032 [==============================] - 20s 10ms/step - loss: 0.2396 - accuracy: 0.8915 - val_loss: 0.8534 - val_accuracy: 0.7174
Epoch 9/20
2032/2032 [==============================] - 18s 9ms/step - loss: 0.2248 - accuracy: 0.8978 - val_loss: 0.9077 - val_accuracy: 0.7145
Epoch 10/20
2032/2032 [==============================] - 20s 10ms/step - loss: 0.2114 - accuracy: 0.9041 - val_loss: 1.0107 - val_accuracy: 0.7182
Epoch 11/20
2032/2032 [==============================] - 20s 10ms/step - loss: 0.2029 - accuracy: 0.9077 - val_loss: 1.0094 - val_accuracy: 0.7157
Epoch 12/20
2032/2032 [==============================] - 20s 10ms/step - loss: 0.1929 - accuracy: 0.9124 - val_loss: 1.0142 - val_accuracy: 0.7110
Epoch 13/20
2032/2032 [==============================] - 20s 10ms/step - loss: 0.1863 - accuracy: 0.9154 - val_loss: 1.1399 - val_accuracy: 0.7173
Epoch 14/20
2032/2032 [==============================] - 20s 10ms/step - loss: 0.1802 - accuracy: 0.9187 - val_loss: 1.1290 - val_accuracy: 0.7141
Epoch 15/20
2032/2032 [==============================] - 18s 9ms/step - loss: 0.1736 - accuracy: 0.9211 - val_loss: 1.1673 - val_accuracy: 0.7122
Epoch 16/20
2032/2032 [==============================] - 18s 9ms/step - loss: 0.1698 - accuracy: 0.9239 - val_loss: 1.1448 - val_accuracy: 0.7139
Epoch 17/20
2032/2032 [==============================] - 21s 10ms/step - loss: 0.1644 - accuracy: 0.9247 - val_loss: 1.2115 - val_accuracy: 0.7137
Epoch 18/20
2032/2032 [==============================] - 20s 10ms/step - loss: 0.1621 - accuracy: 0.9265 - val_loss: 1.2028 - val_accuracy: 0.7140
Epoch 19/20
2032/2032 [==============================] - 20s 10ms/step - loss: 0.1554 - accuracy: 0.9295 - val_loss: 1.3049 - val_accuracy: 0.7084
Epoch 20/20
2032/2032 [==============================] - 21s 10ms/step - loss: 0.1541 - accuracy: 0.9303 - val_loss: 1.2376 - val_accuracy: 0.7110
#accuracy curve 
plt.figure(figsize=(8,6)) # definig the fig size 
plt.plot(history.history['accuracy']) # plotting the accuracy plot
plt.plot(history.history['val_accuracy'])# plotting the validation accuracy graph
plt.legend(['Training_Accuacry', 'Validation_Accuracy'])
plt.xlabel('Number of Epochs') # number of epochs 
plt.ylabel('Traning Accuarcy')
plt.title('Accuracy curve')
plt.show()

#loss curve 
plt.figure(figsize=(8,6))
plt.plot(history.history['loss']) # plotting loss graph
plt.plot(history.history['val_loss'])# plotting validation loss
plt.legend(['Training_Loss', 'Validation_Loss'])# used to provide information in the curve
plt.xlabel('Number of Epochs')
plt.ylabel('LOSS')
plt.title('LOSS curve') 
plt.show()

from keras.layers import LSTM
from tensorflow.keras.layers import Dense
gru_model=Sequential()
#embedding help in encoding the data into integer 
gru_model.add(Embedding(12000, 80,input_length=X_train.shape[1])) # input layer 
gru_model.add(Dropout(0.2)) # this help model from undergoing under fitting or over fitting
gru_model.add(Conv1D(64, kernel_size=3, padding='same', strides=2,  
                    activation='relu'))# add convolution layer for processing the input 
gru_model.add(tf.keras.layers.GRU(128,activation='relu'))
gru_model.add(Dense(128, activation='relu')) # dense layer for producing the output
gru_model.add(Dropout(0.2))
gru_model.add(Dense(5, activation='sigmoid'))
WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
gru_model.compile(optimizer='adam', loss="categorical_crossentropy", metrics=['accuracy'])
with tf.device("/device:GPU:0"):
  gru_history=gru_model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=64, epochs=20, verbose=1, callbacks=[es]) # model training 
Epoch 1/20
2032/2032 [==============================] - 147s 71ms/step - loss: 0.5435 - accuracy: 0.7245 - val_loss: 0.5025 - val_accuracy: 0.7553
Epoch 2/20
2032/2032 [==============================] - 144s 71ms/step - loss: 0.4730 - accuracy: 0.7742 - val_loss: 0.5026 - val_accuracy: 0.7536
Epoch 3/20
2032/2032 [==============================] - 146s 72ms/step - loss: 0.4376 - accuracy: 0.7939 - val_loss: 0.5187 - val_accuracy: 0.7477
Epoch 4/20
2032/2032 [==============================] - 149s 73ms/step - loss: 0.4026 - accuracy: 0.8133 - val_loss: 0.5434 - val_accuracy: 0.7395
Epoch 5/20
2032/2032 [==============================] - 147s 72ms/step - loss: 0.3661 - accuracy: 0.8313 - val_loss: 0.6016 - val_accuracy: 0.7343
Epoch 6/20
2032/2032 [==============================] - 148s 73ms/step - loss: 0.3287 - accuracy: 0.8495 - val_loss: 0.6764 - val_accuracy: 0.7327
Epoch 7/20
2032/2032 [==============================] - 145s 71ms/step - loss: 0.2979 - accuracy: 0.8638 - val_loss: 0.7207 - val_accuracy: 0.7306
Epoch 8/20
2032/2032 [==============================] - 145s 71ms/step - loss: 0.2723 - accuracy: 0.8773 - val_loss: 0.8130 - val_accuracy: 0.7254
Epoch 9/20
2032/2032 [==============================] - 145s 71ms/step - loss: 0.2510 - accuracy: 0.8863 - val_loss: 0.9312 - val_accuracy: 0.7189
Epoch 10/20
2032/2032 [==============================] - 145s 71ms/step - loss: 0.2327 - accuracy: 0.8943 - val_loss: 0.9934 - val_accuracy: 0.7200
Epoch 11/20
2032/2032 [==============================] - 146s 72ms/step - loss: 0.2189 - accuracy: 0.9018 - val_loss: 1.0529 - val_accuracy: 0.7142
Epoch 12/20
2032/2032 [==============================] - 149s 73ms/step - loss: 0.2079 - accuracy: 0.9068 - val_loss: 1.0403 - val_accuracy: 0.7177
Epoch 13/20
2032/2032 [==============================] - 151s 74ms/step - loss: 0.1982 - accuracy: 0.9109 - val_loss: 1.1523 - val_accuracy: 0.7151
Epoch 14/20
2032/2032 [==============================] - 145s 71ms/step - loss: 0.1899 - accuracy: 0.9146 - val_loss: 1.2194 - val_accuracy: 0.7128
Epoch 15/20
2032/2032 [==============================] - 149s 73ms/step - loss: 0.1819 - accuracy: 0.9184 - val_loss: 1.3134 - val_accuracy: 0.7145
Epoch 16/20
2032/2032 [==============================] - 146s 72ms/step - loss: 0.1751 - accuracy: 0.9217 - val_loss: 1.2970 - val_accuracy: 0.7127
Epoch 17/20
2032/2032 [==============================] - 145s 71ms/step - loss: 0.1701 - accuracy: 0.9238 - val_loss: 1.4307 - val_accuracy: 0.7125
Epoch 18/20
2032/2032 [==============================] - 146s 72ms/step - loss: 0.1629 - accuracy: 0.9269 - val_loss: 1.4634 - val_accuracy: 0.7076
Epoch 19/20
2032/2032 [==============================] - 145s 72ms/step - loss: 0.1597 - accuracy: 0.9285 - val_loss: 1.5022 - val_accuracy: 0.7106
Epoch 20/20
2032/2032 [==============================] - 145s 72ms/step - loss: 0.1546 - accuracy: 0.9310 - val_loss: 1.3889 - val_accuracy: 0.7079
#accuracy curve 
plt.figure(figsize=(8,6)) # definig the fig size 
plt.plot(gru_history.history['accuracy']) # plotting the accuracy plot
plt.plot(gru_history.history['val_accuracy'])# plotting the validation accuracy graph
plt.legend(['Training_Accuacry', 'Validation_Accuracy'])
plt.xlabel('Number of Epochs') # number of epochs 
plt.ylabel('Traning Accuarcy')
plt.title('Accuracy curve')
plt.show()

#loss curve 
plt.figure(figsize=(8,6))
plt.plot(gru_history.history['loss']) # plotting loss graph
plt.plot(gru_history.history['val_loss'])# plotting validation loss
plt.legend(['Training_Loss', 'Validation_Loss'])# used to provide information in the curve
plt.xlabel('Number of Epochs')
plt.ylabel('LOSS')
plt.title('LOSS curve') 
plt.show()

LSTM_model=Sequential()
#embedding help in encoding the data into integer 
LSTM_model.add(Embedding(12000, 80,input_length=X_train.shape[1])) # input layer 
LSTM_model.add(Dropout(0.2)) # this help model from undergoing under fitting or over fitting
LSTM_model.add(Conv1D(64, kernel_size=3, padding='same', strides=2,  
                    activation='relu'))# add convolution layer for processing the input 
LSTM_model.add(LSTM(64,activation='relu'))
LSTM_model.add(Dense(128, activation='relu')) # dense layer for producing the output
LSTM_model.add(Dropout(0.2))
LSTM_model.add(Dense(5, activation='sigmoid'))
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
LSTM_model.compile(optimizer='adam', loss="categorical_crossentropy", metrics=['accuracy'])
with tf.device("/device:GPU:0"):
  LSTM_history=LSTM_model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=64, epochs=10, verbose=1, callbacks=[es]) # model training
Epoch 1/10
2032/2032 [==============================] - 128s 62ms/step - loss: 0.5477 - accuracy: 0.7201 - val_loss: 0.5012 - val_accuracy: 0.7539
Epoch 2/10
2032/2032 [==============================] - 120s 59ms/step - loss: 0.4719 - accuracy: 0.7754 - val_loss: 0.5045 - val_accuracy: 0.7521
Epoch 3/10
2032/2032 [==============================] - 121s 60ms/step - loss: 0.4336 - accuracy: 0.7968 - val_loss: 0.5167 - val_accuracy: 0.7476
Epoch 4/10
2032/2032 [==============================] - 121s 59ms/step - loss: 0.3912 - accuracy: 0.8203 - val_loss: 0.5649 - val_accuracy: 0.7415
Epoch 5/10
2032/2032 [==============================] - 120s 59ms/step - loss: 0.3492 - accuracy: 0.8404 - val_loss: 0.6301 - val_accuracy: 0.7361
Epoch 6/10
2032/2032 [==============================] - 121s 59ms/step - loss: 0.3122 - accuracy: 0.8585 - val_loss: 0.6843 - val_accuracy: 0.7278
Epoch 7/10
2032/2032 [==============================] - 118s 58ms/step - loss: 0.2821 - accuracy: 0.8723 - val_loss: 0.8169 - val_accuracy: 0.7265
Epoch 8/10
2032/2032 [==============================] - 117s 58ms/step - loss: 0.2612 - accuracy: 0.8813 - val_loss: 0.8716 - val_accuracy: 0.7254
Epoch 9/10
2032/2032 [==============================] - 116s 57ms/step - loss: 0.2424 - accuracy: 0.8899 - val_loss: 0.9548 - val_accuracy: 0.7219
Epoch 10/10
2032/2032 [==============================] - 116s 57ms/step - loss: 0.2282 - accuracy: 0.8963 - val_loss: 1.0234 - val_accuracy: 0.7214
#loss curve 
plt.figure(figsize=(8,6))
plt.plot(LSTM_history.history['loss']) # plotting loss graph
plt.plot(LSTM_history.history['val_loss'])# plotting validation loss
plt.legend(['Training_Loss', 'Validation_Loss'])# used to provide information in the curve
plt.xlabel('Number of Epochs')
plt.ylabel('LOSS')
plt.title('LOSS curve') 
plt.show()

#loss curve 
plt.figure(figsize=(8,6))
plt.plot(LSTM_history.history['loss']) # plotting loss graph
plt.plot(LSTM_history.history['val_loss'])# plotting validation loss
plt.legend(['Training_Loss', 'Validation_Loss'])# used to provide information in the curve
plt.xlabel('Number of Epochs')
plt.ylabel('LOSS')
plt.title('LOSS curve') 
plt.show()

pred=LSTM_model.predict(X_test)
from sklearn.metrics import classification_report
pred=np.argmax(pred, axis=1)
y_test=np.argmax(y_test, axis=1)
print(classification_report(y_test, pred))
              precision    recall  f1-score   support

           0       0.72      0.73      0.72     34958
           4       0.73      0.71      0.72     35042

    accuracy                           0.72     70000
   macro avg       0.72      0.72      0.72     70000
weighted avg       0.72      0.72      0.72     70000


